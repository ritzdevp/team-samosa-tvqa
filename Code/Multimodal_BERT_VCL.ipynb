{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multimodal_BERT_VCL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritzdevp/team-samosa-tvqa/blob/main/Code/Multimodal_BERT_VCL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "obdPYS3R2J2m",
        "outputId": "18c00e4f-0aa5-4713-842c-329da77261c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Apr 30 23:41:25 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHmm2JWP8dOQ",
        "outputId": "e132371f-cca4-47ed-aa1e-4225d1b6af56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'team-samosa-tvqa'...\n",
            "remote: Enumerating objects: 274, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 274 (delta 11), reused 0 (delta 0), pack-reused 252\u001b[K\n",
            "Receiving objects: 100% (274/274), 2.76 MiB | 14.49 MiB/s, done.\n",
            "Resolving deltas: 100% (148/148), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ritzdevp/team-samosa-tvqa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1m8bC4lefQsP2tRhMLAaiy0AVuBXZtegc' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1m8bC4lefQsP2tRhMLAaiy0AVuBXZtegc\" -O tvqa_imagenet_resnet101_pool5_hq.tar.gz && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuP5ePiy8wwN",
        "outputId": "dd16b783-a309-4b40-c16d-fa306f69991f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-30 23:43:22--  https://docs.google.com/uc?export=download&confirm=t&id=1m8bC4lefQsP2tRhMLAaiy0AVuBXZtegc\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.142.100, 74.125.142.139, 74.125.142.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.142.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0k-6g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9ffl8dnk460lg5mdiarjl1mer9ojl90f/1651362150000/07218467666063224277/*/1m8bC4lefQsP2tRhMLAaiy0AVuBXZtegc?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-04-30 23:43:22--  https://doc-0k-6g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9ffl8dnk460lg5mdiarjl1mer9ojl90f/1651362150000/07218467666063224277/*/1m8bC4lefQsP2tRhMLAaiy0AVuBXZtegc?e=download\n",
            "Resolving doc-0k-6g-docs.googleusercontent.com (doc-0k-6g-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-0k-6g-docs.googleusercontent.com (doc-0k-6g-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36530196109 (34G) [application/x-gzip]\n",
            "Saving to: ‘tvqa_imagenet_resnet101_pool5_hq.tar.gz’\n",
            "\n",
            "tvqa_imagenet_resne 100%[===================>]  34.02G   158MB/s    in 4m 14s  \n",
            "\n",
            "2022-04-30 23:47:36 (137 MB/s) - ‘tvqa_imagenet_resnet101_pool5_hq.tar.gz’ saved [36530196109/36530196109]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! tar xzf /content/tvqa_imagenet_resnet101_pool5_hq.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k79jTHe9-vlX",
        "outputId": "d62dca28-9c82-45ed-aa5b-2f04f1ac4434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets"
      ],
      "metadata": {
        "id": "hDG6HKbZ8xJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "281bdcc0-b9e2-4653-a006-bbe312a7a783"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.0 MB 9.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 325 kB 47.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 43.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 49.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 45.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 44.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 136 kB 56.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 54.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 48.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 27 kB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 50.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 53.2 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import BertTokenizer, VisualBertForQuestionAnswering, VisualBertModel"
      ],
      "metadata": {
        "id": "2-tsiNF08_bE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",  output_hidden_states = True)\n",
        "bert_model = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\",  output_hidden_states = True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8wxe3bm__p0",
        "outputId": "c1bb186d-2958-424a-bc45-9a7f88f456ce"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at uclanlp/visualbert-vqa-coco-pre were not used when initializing VisualBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "% cd /content/team-samosa-tvqa/baseline_repo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq8Nl5G4bncZ",
        "outputId": "2a6d66cd-d0f7-468b-eeaa-a18a7a924018"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/team-samosa-tvqa/baseline_repo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 777 /content/team-samosa-tvqa/baseline_repo/setup.sh"
      ],
      "metadata": {
        "id": "B6YhEaysbsyg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! /content/team-samosa-tvqa/baseline_repo/setup.sh"
      ],
      "metadata": {
        "id": "ULuNTvbjbu22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb1e7ecf-f16b-48e1-ade4-79695c5079e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-01 00:07:55--  https://tvqa.cs.unc.edu/files/tvqa_qa_release.tar.gz\n",
            "Resolving tvqa.cs.unc.edu (tvqa.cs.unc.edu)... 152.2.132.230\n",
            "Connecting to tvqa.cs.unc.edu (tvqa.cs.unc.edu)|152.2.132.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14474003 (14M) [application/x-gzip]\n",
            "Saving to: ‘/content/tvqa_qa_release.tar.gz.1’\n",
            "\n",
            "tvqa_qa_release.tar 100%[===================>]  13.80M  17.8MB/s    in 0.8s    \n",
            "\n",
            "2022-05-01 00:07:56 (17.8 MB/s) - ‘/content/tvqa_qa_release.tar.gz.1’ saved [14474003/14474003]\n",
            "\n",
            "--2022-05-01 00:07:56--  https://tvqa.cs.unc.edu/files/tvqa_subtitles.tar.gz\n",
            "Resolving tvqa.cs.unc.edu (tvqa.cs.unc.edu)... 152.2.132.230\n",
            "Connecting to tvqa.cs.unc.edu (tvqa.cs.unc.edu)|152.2.132.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15495443 (15M) [application/x-gzip]\n",
            "Saving to: ‘/content/tvqa_subtitles.tar.gz’\n",
            "\n",
            "tvqa_subtitles.tar. 100%[===================>]  14.78M  18.8MB/s    in 0.8s    \n",
            "\n",
            "2022-05-01 00:08:00 (18.8 MB/s) - ‘/content/tvqa_subtitles.tar.gz’ saved [15495443/15495443]\n",
            "\n",
            "--2022-05-01 00:08:01--  https://tvqa.cs.unc.edu/files/tvqa_plus_annotations.tar.gz\n",
            "Resolving tvqa.cs.unc.edu (tvqa.cs.unc.edu)... 152.2.132.230\n",
            "Connecting to tvqa.cs.unc.edu (tvqa.cs.unc.edu)|152.2.132.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6718915 (6.4M) [application/x-gzip]\n",
            "Saving to: ‘/content/tvqa_plus/tvqa_plus_annotations.tar.gz’\n",
            "\n",
            "tvqa_plus_annotatio 100%[===================>]   6.41M  10.4MB/s    in 0.6s    \n",
            "\n",
            "2022-05-01 00:08:02 (10.4 MB/s) - ‘/content/tvqa_plus/tvqa_plus_annotations.tar.gz’ saved [6718915/6718915]\n",
            "\n",
            "--2022-05-01 00:08:02--  https://tvqa.cs.unc.edu/files/tvqa_plus_annotations_preproc_with_test.tar.gz\n",
            "Resolving tvqa.cs.unc.edu (tvqa.cs.unc.edu)... 152.2.132.230\n",
            "Connecting to tvqa.cs.unc.edu (tvqa.cs.unc.edu)|152.2.132.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7323926 (7.0M) [application/x-gzip]\n",
            "Saving to: ‘/content/tvqa_plus/tvqa_plus_annotations_preproc_with_test.tar.gz’\n",
            "\n",
            "tvqa_plus_annotatio 100%[===================>]   6.98M  11.2MB/s    in 0.6s    \n",
            "\n",
            "2022-05-01 00:08:03 (11.2 MB/s) - ‘/content/tvqa_plus/tvqa_plus_annotations_preproc_with_test.tar.gz’ saved [7323926/7323926]\n",
            "\n",
            "--2022-05-01 00:08:04--  https://tvqa.cs.unc.edu/files/tvqa_plus_subtitles.tar.gz\n",
            "Resolving tvqa.cs.unc.edu (tvqa.cs.unc.edu)... 152.2.132.230\n",
            "Connecting to tvqa.cs.unc.edu (tvqa.cs.unc.edu)|152.2.132.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1731355 (1.7M) [application/x-gzip]\n",
            "Saving to: ‘/content/tvqa_plus/tvqa_plus_subtitles.tar.gz’\n",
            "\n",
            "tvqa_plus_subtitles 100%[===================>]   1.65M  3.95MB/s    in 0.4s    \n",
            "\n",
            "2022-05-01 00:08:04 (3.95 MB/s) - ‘/content/tvqa_plus/tvqa_plus_subtitles.tar.gz’ saved [1731355/1731355]\n",
            "\n",
            "--2022-05-01 00:08:04--  http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/wordvecs/glove.6B.zip [following]\n",
            "--2022-05-01 00:08:04--  https://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip [following]\n",
            "--2022-05-01 00:08:05--  http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182753 (822M) [application/zip]\n",
            "Saving to: ‘/content/glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 40s  \n",
            "\n",
            "2022-05-01 00:10:44 (5.15 MB/s) - ‘/content/glove.6B.zip’ saved [862182753/862182753]\n",
            "\n",
            "caution: filename not matched:  -C\n",
            "caution: filename not matched:  /content\n",
            "Collecting pysrt\n",
            "  Downloading pysrt-1.1.2.tar.gz (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pysrt) (3.0.4)\n",
            "Building wheels for collected packages: pysrt\n",
            "  Building wheel for pysrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pysrt: filename=pysrt-1.1.2-py3-none-any.whl size=13444 sha256=8064d13c2c2181216fca24f7cfdf933dfa8581c02df82ce9caeb8ef9f38dc572\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/f8/e8/a26be4111ab5ec931e845777e574d1483b4adddc50d3e591a6\n",
            "Successfully built pysrt\n",
            "Installing collected packages: pysrt\n",
            "Successfully installed pysrt-1.1.2\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.3.0)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.15-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 8.4 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 56.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.10-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 58.0 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.25.11)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=ed0b6da2b541af9d16582473a8c4f6f1c60ea8372ab2768a13d59ba860066fda\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.10 setproctitle-1.2.3 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://tvqa.cs.unc.edu/files/det_visual_concepts_hq.pickle.tar.gz\n",
        "!tar -xvf det_visual_concepts_hq.pickle.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yIc2Zub5gvU",
        "outputId": "6b3f1870-0252-4d10-db83-1336f01ce959"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-01 00:11:16--  http://tvqa.cs.unc.edu/files/det_visual_concepts_hq.pickle.tar.gz\n",
            "Resolving tvqa.cs.unc.edu (tvqa.cs.unc.edu)... 152.2.132.230\n",
            "Connecting to tvqa.cs.unc.edu (tvqa.cs.unc.edu)|152.2.132.230|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 100718745 (96M) [application/x-gzip]\n",
            "Saving to: ‘det_visual_concepts_hq.pickle.tar.gz’\n",
            "\n",
            "det_visual_concepts 100%[===================>]  96.05M  26.1MB/s    in 5.9s    \n",
            "\n",
            "2022-05-01 00:11:25 (16.2 MB/s) - ‘det_visual_concepts_hq.pickle.tar.gz’ saved [100718745/100718745]\n",
            "\n",
            "det_visual_concepts_hq.pickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-pv72vr9hXd",
        "outputId": "50c7f071-7f71-4caf-ed0e-2aa732eddcaa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/glove.6B.zip\n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "  inflating: glove.6B.50d.txt        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VCL FILE"
      ],
      "metadata": {
        "id": "KYj4Ad_O58Xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwordslist = stopwords.words('english')\n",
        "stopwordslist.append(\"?\")\n",
        "exceptlist = [\"what\", \"who\", \"whom\", \"how\", \"where\", \"why\"]\n",
        "for w in exceptlist:\n",
        "  stopwordslist.remove(w)\n",
        "\n",
        "vis_concepts = pickle.load(open('/content/det_visual_concepts_hq.pickle', 'rb'))\n",
        "\n",
        "embedding_index = {}\n",
        "f = open('/content/glove.6B.50d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:],dtype='float32')\n",
        "    embedding_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "def get_detected_objects(episode, index):\n",
        "  x = vis_concepts[episode][index].strip().split(\",\")\n",
        "  y = []\n",
        "  for w in x:\n",
        "    y.append(w.strip())\n",
        "\n",
        "  return y\n",
        "\n",
        "def phrase_splitter(arr):\n",
        "  new_arr = []\n",
        "  for w in arr:\n",
        "    temp = w.split()\n",
        "    for t in temp:\n",
        "      if (t not in new_arr):\n",
        "        new_arr.append(t)\n",
        "  return new_arr\n",
        "\n",
        "def get_glove_wordlist_embedding(arr_of_words):\n",
        "  glove_wordlist_embedding = []\n",
        "  for w in arr_of_words:\n",
        "    if (w in embedding_index):\n",
        "      glove_wordlist_embedding.append(embedding_index[w])\n",
        "  glove_wordlist_embedding = np.array(glove_wordlist_embedding)\n",
        "  return glove_wordlist_embedding\n",
        "\n",
        "\n",
        "def tokenize_question(question):\n",
        "  words = nltk.word_tokenize(question)\n",
        "  filtered_words = [word.lower() for word in words if word.lower() not in stopwordslist ]\n",
        "  qarr = []\n",
        "  for w in filtered_words:\n",
        "    if w in embedding_index:\n",
        "      qarr.append(w)\n",
        "  return qarr\n",
        "\n",
        "def relevance_score(episode, frame_index, question):\n",
        "  #function to get relevance score between question and detected objects in a frame\n",
        "  detected_objects_list = get_detected_objects(episode, frame_index)[:10]\n",
        "  words = nltk.word_tokenize(question)\n",
        "\n",
        "  #to split words like \"pink shirt\" to \"pink\" and \"shirt\"\n",
        "  detected_objects_list = phrase_splitter(detected_objects_list)\n",
        "\n",
        "  #num_det is number of words in detected objects list\n",
        "  #num_q is number of words in question\n",
        "  det_objs_embedding_matrix = get_glove_wordlist_embedding(detected_objects_list) #num_det x 50\n",
        "  question_embedding_matrix = get_glove_wordlist_embedding(tokenize_question(question)) #num_q x 50\n",
        "\n",
        "  if (len(detected_objects_list) == 0):\n",
        "    return np.zeros(10), 0\n",
        "\n",
        "  score_matrix = det_objs_embedding_matrix.dot(question_embedding_matrix.T)  #num_det x num_q\n",
        "  score = np.sum(score_matrix)\n",
        "\n",
        "  return score_matrix, score\n",
        "\n",
        "### returns maximum sum subarray of given window size\n",
        "\n",
        "def get_maxsum_subarray(myarr, window_size):\n",
        "  start = 0\n",
        "  sum = 0\n",
        "  currlen = 0\n",
        "  ws = window_size\n",
        "  maxsum = 0\n",
        "  maxindexend = -1\n",
        "  for i in range(len(myarr) - ws):\n",
        "    sum += myarr[i]\n",
        "    currlen += 1\n",
        "    if (currlen > ws):\n",
        "      sum -= myarr[i - ws]\n",
        "      currlen -= 1\n",
        "    \n",
        "    if (sum > maxsum):\n",
        "      maxsum = sum\n",
        "      maxindexend = i\n",
        "  \n",
        "  maxstartindex = maxindexend - ws + 1\n",
        "  return maxstartindex, maxsum\n",
        "\n",
        "\n",
        "### returns relevance scores of all frames for the given question\n",
        "def get_relevance_scorelist(vid_name, question):\n",
        "  num_frames = len(vis_concepts[vid_name])\n",
        "  score_list = np.zeros(num_frames)\n",
        "\n",
        "  max_score_index = -1\n",
        "  max_score = 0\n",
        "  min_score_index = -1\n",
        "  min_score = 10000\n",
        "\n",
        "  for frame_index in range(num_frames):\n",
        "    score_matrix, score = relevance_score(vid_name, frame_index, question)\n",
        "    score_list[frame_index] = score\n",
        "    if (score > max_score):\n",
        "      max_score = score\n",
        "      max_score_index = frame_index\n",
        "    if (score < min_score):\n",
        "      min_score = score\n",
        "      min_score_index = frame_index\n",
        "\n",
        "  return score_list, max_score_index, min_score_index\n",
        "\n",
        "def make_link(vid_name, frame_index):\n",
        "  dummy = \"https://tvqammml.s3.us-east-1.amazonaws.com/\"\n",
        "  temp = \"00000\"\n",
        "  temp = (temp + str(frame_index))[-5:]\n",
        "  url = dummy + vid_name + '/' + temp + '.jpg'\n",
        "  return url\n",
        "\n",
        "def plot_image_from_url(url):\n",
        "  image = io.imread(url)\n",
        "  plt.imshow(image)\n",
        "  plt.show()\n",
        "\n",
        "def plot_max_relevance_frames(vid_name, start_index, window_size):\n",
        "  dummy = \"https://tvqammml.s3.us-east-1.amazonaws.com/\"\n",
        "  fig = plt.figure(figsize=(10, 20))\n",
        "  columns = 2\n",
        "  rows = window_size/2\n",
        "  j = start_index\n",
        "  for i in range(1, window_size +1):\n",
        "      temp = \"00000\"\n",
        "      temp = (temp + str(j))[-5:]\n",
        "      url = dummy + vid_name + '/' + temp + '.jpg'\n",
        "      img = io.imread(url)\n",
        "      fig.add_subplot(rows, columns, i)\n",
        "      plt.imshow(img)\n",
        "      j += 1\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Returns the unique objects detected in an episode from\n",
        "a start frame index, given the window size\n",
        "\"\"\"\n",
        "\n",
        "def get_unique_objs_in_chunk(episode, start_index, window_size):\n",
        "  num_frames = len(vis_concepts[episode])\n",
        "  temp = set()\n",
        "  for i in range(start_index, start_index + window_size, 1):\n",
        "    arr = get_detected_objects(episode, i)\n",
        "    arr = phrase_splitter(arr)\n",
        "    for w in arr:\n",
        "      temp.add(w)\n",
        "  return list(temp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nuG8CBC58Gy",
        "outputId": "52ef49a2-e6af-4162-8816-7ff77914a652"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#debugging\n",
        "import h5py\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from CONSTANTS import RESNET_FEATURES\n",
        "\n",
        "h5driver=None\n",
        "vid_feat_path=RESNET_FEATURES\n",
        "vid_h5 = h5py.File(vid_feat_path, \"r\", driver=h5driver)\n",
        "\n",
        "def read_resnet_feats(video_names, questions, stride=8, VCL_window_size=10):\n",
        "    video_resnet_feat = []\n",
        "    unique_objects_per_vid = []\n",
        "    assert len(video_names) == len(questions)\n",
        "    for i, video in enumerate(video_names):\n",
        "        quest_i = questions[i]\n",
        "        score_list, max_score_index, min_score_index = get_relevance_scorelist(video, quest_i)\n",
        "        start_index, max_score = get_maxsum_subarray(score_list, VCL_window_size)\n",
        "        unique_objects = get_unique_objs_in_chunk(video, start_index, VCL_window_size)\n",
        "        \n",
        "        unique_objects_per_vid.append(' '.join(unique_objects))\n",
        "        video_resnet_feat.append(torch.tensor(vid_h5[video][start_index:start_index+VCL_window_size, :], device=\"cuda\"))\n",
        "\n",
        "    video_resnet_feat =  pad_sequence(video_resnet_feat, batch_first=True)\n",
        "    \n",
        "    return video_resnet_feat, unique_objects_per_vid"
      ],
      "metadata": {
        "id": "KYx7JNj1biEm"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TVQA_Multimodal(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(TVQA_Multimodal, self).__init__()\n",
        "\n",
        "        self.bert_tokenizer = bert_tokenizer\n",
        "        self.bert_model = bert_model.to(\"cuda\")\n",
        "\n",
        "        self.proj = torch.nn.Sequential(torch.nn.Linear(768, 256),\n",
        "                                        torch.nn.GELU(),\n",
        "                                        torch.nn.Linear(256, 64),\n",
        "                                        torch.nn.GELU(),\n",
        "                                        torch.nn.Linear(64, 1)).to(\"cuda\")\n",
        "\n",
        "    def _freeze_bert_weights(self, requires_grad_layers = ['encoder.layer.11',\n",
        "                                                           'pooler.dense.weight',\n",
        "                                                           'pooler.dense.bias']):\n",
        "        for name, p in self.bert_model.named_parameters():\n",
        "            req_grad = False\n",
        "            for layer_name in requires_grad_layers:\n",
        "                if layer_name in name:\n",
        "                    req_grad = True\n",
        "                    break\n",
        "\n",
        "            p.requires_grad = req_grad\n",
        "        \n",
        "        return True\n",
        "\n",
        "    def merge_q_ans(self, que, ans):\n",
        "        merged = []\n",
        "        for i in range(len(que)):\n",
        "          curr = que[i]+ ' [SEP] ' + ans[i]\n",
        "          merged.append(curr)\n",
        "        return merged\n",
        "\n",
        "    def merge_subt_visconcepts(self, subtitles, unique_objects):\n",
        "        merged = []\n",
        "        for i in range(len(subtitles)):\n",
        "          curr = subtitles[i]+ ' [SEP] ' + unique_objects[i]\n",
        "          merged.append(curr)\n",
        "        return merged\n",
        "\n",
        "\n",
        "    def bert_forward(self, question, ans, subt_text, video_names):\n",
        "        #all arguments have batch size\n",
        "        visual_embeds, unique_objs_per_vid = read_resnet_feats(video_names, question)\n",
        "        merge_q_ans = self.merge_q_ans(question, ans)\n",
        "        merge_sub_vc = self.merge_subt_visconcepts(subt_text, unique_objs_per_vid)\n",
        "\n",
        "        # print(\"merge sub vc\", merge_sub_vc)\n",
        "        # print(\"merge sub vc 0\", merge_sub_vc[0])\n",
        "        \n",
        "\n",
        "        inputs       = bert_tokenizer(merge_sub_vc, \n",
        "                                      merge_q_ans,  \n",
        "                                      padding=\"max_length\",\n",
        "                                      truncation=True,\n",
        "                                      return_token_type_ids=True,\n",
        "                                      return_attention_mask=True,\n",
        "                                      add_special_tokens=True,\n",
        "                                      return_tensors=\"pt\")\n",
        "        \n",
        "        \n",
        "\n",
        "        # print(\"---subt text --\", len(subt_text))\n",
        "        # print(\"---unique_objs_per_vid--\", len(unique_objs_per_vid))\n",
        "        # print('vis embed shape', visual_embeds.shape)\n",
        "        # print('unique obj', unique_objs_per_vid)\n",
        "        # print(\"--subt--\", subt_text)\n",
        "        \n",
        "\n",
        "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).to(\"cuda\")\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float).to(\"cuda\")\n",
        "        inputs.update(\n",
        "            {\n",
        "                \"visual_embeds\": visual_embeds,\n",
        "                \"visual_token_type_ids\": visual_token_type_ids,\n",
        "                \"visual_attention_mask\": visual_attention_mask,\n",
        "            }\n",
        "        )\n",
        "        inputs = inputs.to(\"cuda\")\n",
        "\n",
        "        output = bert_model(**inputs)\n",
        "\n",
        "        hidden_states = output.last_hidden_state\n",
        "        cls_tokens = hidden_states[:,0,:]\n",
        "\n",
        "        return cls_tokens\n",
        "\n",
        "    def forward(self, question, subt_text, a0, a1, a2, a3, a4, video_names):\n",
        "\n",
        "        bert_a0 = self.bert_forward(question=question,\n",
        "                                    ans=a0,\n",
        "                                    subt_text=subt_text,\n",
        "                                    video_names=video_names)        \n",
        "        score_a0 = self.proj(bert_a0)\n",
        "\n",
        "        bert_a1 = self.bert_forward(question=question,\n",
        "                                    ans=a1,\n",
        "                                    subt_text=subt_text,\n",
        "                                    video_names=video_names)\n",
        "        score_a1 = self.proj(bert_a1)\n",
        "\n",
        "        bert_a2 = self.bert_forward(question=question,\n",
        "                                    ans=a2,\n",
        "                                    subt_text=subt_text,\n",
        "                                    video_names=video_names)\n",
        "        score_a2 = self.proj(bert_a2)\n",
        "\n",
        "        bert_a3 = self.bert_forward(question=question,\n",
        "                                    ans=a3,\n",
        "                                    subt_text=subt_text,\n",
        "                                    video_names=video_names)\n",
        "        \n",
        "        score_a3 = self.proj(bert_a3)\n",
        "\n",
        "        bert_a4 = self.bert_forward(question=question,\n",
        "                                    ans=a4,\n",
        "                                    subt_text=subt_text,\n",
        "                                    video_names=video_names)\n",
        "        \n",
        "        score_a4 = self.proj(bert_a4)\n",
        "\n",
        "        # print(\" score_a4 \", score_a4.shape)\n",
        "\n",
        "\n",
        "        logits = torch.cat((score_a0, score_a1, score_a2, score_a3, score_a4), dim=1)\n",
        "\n",
        "        # print(\" logits \", logits.shape)\n",
        "\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "1vAyQMnp9GBh"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from CONSTANTS import  BASE_PATH\n",
        "import json"
      ],
      "metadata": {
        "id": "XYnj5IN1MtqZ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TVQAPlus(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset='train'):\n",
        "        # sample represent how many npy files will be preloaded for one __getitem__ call\n",
        "        TRAIN_DICT_JSON = BASE_PATH + \"/tvqa_plus/tvqa_plus_train.json\"\n",
        "        VAL_DICT_JSON = BASE_PATH + '/tvqa_plus/tvqa_plus_val.json'\n",
        "        SUBTITLES_DICT_JSON = BASE_PATH + '/tvqa_plus/tvqa_plus_subtitles.json'\n",
        "\n",
        "        self.dataset = dataset\n",
        "\n",
        "        train_dict = {}\n",
        "        val_dict = {}\n",
        "        subtitles_dict = {}\n",
        "\n",
        "        with open(TRAIN_DICT_JSON) as f:\n",
        "          train_dict = json.load(f)\n",
        "\n",
        "        with open(VAL_DICT_JSON) as f:\n",
        "          val_dict = json.load(f)\n",
        "\n",
        "        with open(SUBTITLES_DICT_JSON) as f:\n",
        "          self.subtitles_dict = json.load(f)\n",
        "\n",
        "        self.target_dict = {}\n",
        "        if self.dataset == 'train':\n",
        "          self.target_dict = train_dict\n",
        "        elif self.dataset == 'val':\n",
        "          self.target_dict = val_dict\n",
        "        else:\n",
        "          raise Exception(f\"Invalid dataset passed {self.dataset}\")\n",
        "      \n",
        "    def __len__(self):\n",
        "        return len(self.target_dict)\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "\n",
        "      question = self.target_dict[i]['q']\n",
        "    \n",
        "      a0       = self.target_dict[i]['a0']\n",
        "      a1       = self.target_dict[i]['a1']\n",
        "      a2       = self.target_dict[i]['a2']\n",
        "      a3       = self.target_dict[i]['a3']\n",
        "      a4       = self.target_dict[i]['a4']\n",
        "\n",
        "      answer_idx = int(self.target_dict[i]['answer_idx'])\n",
        "      \n",
        "      video_name = self.target_dict[i]['vid_name']\n",
        "      subtitle = self.subtitles_dict[video_name]\n",
        "      subt_text = subtitle['sub_text']\n",
        "\n",
        "      return question, subt_text, a0, a1, a2, a3, a4, video_name, answer_idx"
      ],
      "metadata": {
        "id": "c-U-TdY7G32d"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TVQAPlus('train')\n",
        "test_dataset = TVQAPlus(\"val\")"
      ],
      "metadata": {
        "id": "049VKQuMN5hZ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tvqa_model = TVQA_Multimodal()\n",
        "tvqa_model._freeze_bert_weights()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9X8jep2UCndK",
        "outputId": "4c036938-98b6-4948-a96c-283ac9bbb5e8"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=24\n",
        "dev_batch_size=12"
      ],
      "metadata": {
        "id": "Wlc_tIBlFaGF"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=dev_batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "J3jFAMYXCq35"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for batch_idx, ( question, subt_text, a0, a1, a2, a3, a4, video_names, answer_idx) in enumerate(train_loader):\n",
        "#     logits = tvqa_model.forward(question, subt_text, a0, a1, a2, a3, a4, video_names)\n",
        "#     break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p--r0ITmCXB9",
        "outputId": "dd7205ca-635d-4529-d3b2-a5a8e0f8616b"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ishB5PCRlfgn"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tAGjKdomOMPU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge(que, ans):\n",
        "  merged = []\n",
        "  for i in range(len(que)):\n",
        "    curr = que[i]+ ' [SEP] ' + ans[i]\n",
        "    merged.append(curr)\n",
        "  return merged"
      ],
      "metadata": {
        "id": "_gdg1GHUPuwS"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tvqa_model = TVQA_Multimodal()\n",
        "# tvqa_model._freeze_bert_weights()\n",
        "\n",
        "print(tvqa_model.named_parameters())\n",
        "\n",
        "for name, p in tvqa_model.named_parameters():\n",
        "    print(\"name\", name, \"p\", p.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQacAgViSOsi",
        "outputId": "02173381-7eaf-4d29-d74f-730bc6d196e2"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object Module.named_parameters at 0x7f7afde31b50>\n",
            "name bert_model.embeddings.word_embeddings.weight p False\n",
            "name bert_model.embeddings.position_embeddings.weight p False\n",
            "name bert_model.embeddings.token_type_embeddings.weight p False\n",
            "name bert_model.embeddings.LayerNorm.weight p False\n",
            "name bert_model.embeddings.LayerNorm.bias p False\n",
            "name bert_model.embeddings.visual_token_type_embeddings.weight p False\n",
            "name bert_model.embeddings.visual_position_embeddings.weight p False\n",
            "name bert_model.embeddings.visual_projection.weight p False\n",
            "name bert_model.embeddings.visual_projection.bias p False\n",
            "name bert_model.encoder.layer.0.attention.self.query.weight p False\n",
            "name bert_model.encoder.layer.0.attention.self.query.bias p False\n",
            "name bert_model.encoder.layer.0.attention.self.key.weight p False\n",
            "name bert_model.encoder.layer.0.attention.self.key.bias p False\n",
            "name bert_model.encoder.layer.0.attention.self.value.weight p False\n",
            "name bert_model.encoder.layer.0.attention.self.value.bias p False\n",
            "name bert_model.encoder.layer.0.attention.output.dense.weight p False\n",
            "name bert_model.encoder.layer.0.attention.output.dense.bias p False\n",
            "name bert_model.encoder.layer.0.attention.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.0.attention.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.0.intermediate.dense.weight p False\n",
            "name bert_model.encoder.layer.0.intermediate.dense.bias p False\n",
            "name bert_model.encoder.layer.0.output.dense.weight p False\n",
            "name bert_model.encoder.layer.0.output.dense.bias p False\n",
            "name bert_model.encoder.layer.0.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.0.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.1.attention.self.query.weight p False\n",
            "name bert_model.encoder.layer.1.attention.self.query.bias p False\n",
            "name bert_model.encoder.layer.1.attention.self.key.weight p False\n",
            "name bert_model.encoder.layer.1.attention.self.key.bias p False\n",
            "name bert_model.encoder.layer.1.attention.self.value.weight p False\n",
            "name bert_model.encoder.layer.1.attention.self.value.bias p False\n",
            "name bert_model.encoder.layer.1.attention.output.dense.weight p False\n",
            "name bert_model.encoder.layer.1.attention.output.dense.bias p False\n",
            "name bert_model.encoder.layer.1.attention.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.1.attention.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.1.intermediate.dense.weight p False\n",
            "name bert_model.encoder.layer.1.intermediate.dense.bias p False\n",
            "name bert_model.encoder.layer.1.output.dense.weight p False\n",
            "name bert_model.encoder.layer.1.output.dense.bias p False\n",
            "name bert_model.encoder.layer.1.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.1.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.2.attention.self.query.weight p False\n",
            "name bert_model.encoder.layer.2.attention.self.query.bias p False\n",
            "name bert_model.encoder.layer.2.attention.self.key.weight p False\n",
            "name bert_model.encoder.layer.2.attention.self.key.bias p False\n",
            "name bert_model.encoder.layer.2.attention.self.value.weight p False\n",
            "name bert_model.encoder.layer.2.attention.self.value.bias p False\n",
            "name bert_model.encoder.layer.2.attention.output.dense.weight p False\n",
            "name bert_model.encoder.layer.2.attention.output.dense.bias p False\n",
            "name bert_model.encoder.layer.2.attention.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.2.attention.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.2.intermediate.dense.weight p False\n",
            "name bert_model.encoder.layer.2.intermediate.dense.bias p False\n",
            "name bert_model.encoder.layer.2.output.dense.weight p False\n",
            "name bert_model.encoder.layer.2.output.dense.bias p False\n",
            "name bert_model.encoder.layer.2.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.2.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.3.attention.self.query.weight p False\n",
            "name bert_model.encoder.layer.3.attention.self.query.bias p False\n",
            "name bert_model.encoder.layer.3.attention.self.key.weight p False\n",
            "name bert_model.encoder.layer.3.attention.self.key.bias p False\n",
            "name bert_model.encoder.layer.3.attention.self.value.weight p False\n",
            "name bert_model.encoder.layer.3.attention.self.value.bias p False\n",
            "name bert_model.encoder.layer.3.attention.output.dense.weight p False\n",
            "name bert_model.encoder.layer.3.attention.output.dense.bias p False\n",
            "name bert_model.encoder.layer.3.attention.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.3.attention.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.3.intermediate.dense.weight p False\n",
            "name bert_model.encoder.layer.3.intermediate.dense.bias p False\n",
            "name bert_model.encoder.layer.3.output.dense.weight p False\n",
            "name bert_model.encoder.layer.3.output.dense.bias p False\n",
            "name bert_model.encoder.layer.3.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.3.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.4.attention.self.query.weight p False\n",
            "name bert_model.encoder.layer.4.attention.self.query.bias p False\n",
            "name bert_model.encoder.layer.4.attention.self.key.weight p False\n",
            "name bert_model.encoder.layer.4.attention.self.key.bias p False\n",
            "name bert_model.encoder.layer.4.attention.self.value.weight p False\n",
            "name bert_model.encoder.layer.4.attention.self.value.bias p False\n",
            "name bert_model.encoder.layer.4.attention.output.dense.weight p False\n",
            "name bert_model.encoder.layer.4.attention.output.dense.bias p False\n",
            "name bert_model.encoder.layer.4.attention.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.4.attention.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.4.intermediate.dense.weight p False\n",
            "name bert_model.encoder.layer.4.intermediate.dense.bias p False\n",
            "name bert_model.encoder.layer.4.output.dense.weight p False\n",
            "name bert_model.encoder.layer.4.output.dense.bias p False\n",
            "name bert_model.encoder.layer.4.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.4.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.5.attention.self.query.weight p False\n",
            "name bert_model.encoder.layer.5.attention.self.query.bias p False\n",
            "name bert_model.encoder.layer.5.attention.self.key.weight p False\n",
            "name bert_model.encoder.layer.5.attention.self.key.bias p False\n",
            "name bert_model.encoder.layer.5.attention.self.value.weight p False\n",
            "name bert_model.encoder.layer.5.attention.self.value.bias p False\n",
            "name bert_model.encoder.layer.5.attention.output.dense.weight p False\n",
            "name bert_model.encoder.layer.5.attention.output.dense.bias p False\n",
            "name bert_model.encoder.layer.5.attention.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.5.attention.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.5.intermediate.dense.weight p False\n",
            "name bert_model.encoder.layer.5.intermediate.dense.bias p False\n",
            "name bert_model.encoder.layer.5.output.dense.weight p False\n",
            "name bert_model.encoder.layer.5.output.dense.bias p False\n",
            "name bert_model.encoder.layer.5.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.5.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.6.attention.self.query.weight p False\n",
            "name bert_model.encoder.layer.6.attention.self.query.bias p False\n",
            "name bert_model.encoder.layer.6.attention.self.key.weight p False\n",
            "name bert_model.encoder.layer.6.attention.self.key.bias p False\n",
            "name bert_model.encoder.layer.6.attention.self.value.weight p False\n",
            "name bert_model.encoder.layer.6.attention.self.value.bias p False\n",
            "name bert_model.encoder.layer.6.attention.output.dense.weight p False\n",
            "name bert_model.encoder.layer.6.attention.output.dense.bias p False\n",
            "name bert_model.encoder.layer.6.attention.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.6.attention.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.6.intermediate.dense.weight p False\n",
            "name bert_model.encoder.layer.6.intermediate.dense.bias p False\n",
            "name bert_model.encoder.layer.6.output.dense.weight p False\n",
            "name bert_model.encoder.layer.6.output.dense.bias p False\n",
            "name bert_model.encoder.layer.6.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.6.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.7.attention.self.query.weight p False\n",
            "name bert_model.encoder.layer.7.attention.self.query.bias p False\n",
            "name bert_model.encoder.layer.7.attention.self.key.weight p False\n",
            "name bert_model.encoder.layer.7.attention.self.key.bias p False\n",
            "name bert_model.encoder.layer.7.attention.self.value.weight p False\n",
            "name bert_model.encoder.layer.7.attention.self.value.bias p False\n",
            "name bert_model.encoder.layer.7.attention.output.dense.weight p False\n",
            "name bert_model.encoder.layer.7.attention.output.dense.bias p False\n",
            "name bert_model.encoder.layer.7.attention.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.7.attention.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.7.intermediate.dense.weight p False\n",
            "name bert_model.encoder.layer.7.intermediate.dense.bias p False\n",
            "name bert_model.encoder.layer.7.output.dense.weight p False\n",
            "name bert_model.encoder.layer.7.output.dense.bias p False\n",
            "name bert_model.encoder.layer.7.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.7.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.8.attention.self.query.weight p False\n",
            "name bert_model.encoder.layer.8.attention.self.query.bias p False\n",
            "name bert_model.encoder.layer.8.attention.self.key.weight p False\n",
            "name bert_model.encoder.layer.8.attention.self.key.bias p False\n",
            "name bert_model.encoder.layer.8.attention.self.value.weight p False\n",
            "name bert_model.encoder.layer.8.attention.self.value.bias p False\n",
            "name bert_model.encoder.layer.8.attention.output.dense.weight p False\n",
            "name bert_model.encoder.layer.8.attention.output.dense.bias p False\n",
            "name bert_model.encoder.layer.8.attention.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.8.attention.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.8.intermediate.dense.weight p False\n",
            "name bert_model.encoder.layer.8.intermediate.dense.bias p False\n",
            "name bert_model.encoder.layer.8.output.dense.weight p False\n",
            "name bert_model.encoder.layer.8.output.dense.bias p False\n",
            "name bert_model.encoder.layer.8.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.8.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.9.attention.self.query.weight p False\n",
            "name bert_model.encoder.layer.9.attention.self.query.bias p False\n",
            "name bert_model.encoder.layer.9.attention.self.key.weight p False\n",
            "name bert_model.encoder.layer.9.attention.self.key.bias p False\n",
            "name bert_model.encoder.layer.9.attention.self.value.weight p False\n",
            "name bert_model.encoder.layer.9.attention.self.value.bias p False\n",
            "name bert_model.encoder.layer.9.attention.output.dense.weight p False\n",
            "name bert_model.encoder.layer.9.attention.output.dense.bias p False\n",
            "name bert_model.encoder.layer.9.attention.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.9.attention.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.9.intermediate.dense.weight p False\n",
            "name bert_model.encoder.layer.9.intermediate.dense.bias p False\n",
            "name bert_model.encoder.layer.9.output.dense.weight p False\n",
            "name bert_model.encoder.layer.9.output.dense.bias p False\n",
            "name bert_model.encoder.layer.9.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.9.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.10.attention.self.query.weight p False\n",
            "name bert_model.encoder.layer.10.attention.self.query.bias p False\n",
            "name bert_model.encoder.layer.10.attention.self.key.weight p False\n",
            "name bert_model.encoder.layer.10.attention.self.key.bias p False\n",
            "name bert_model.encoder.layer.10.attention.self.value.weight p False\n",
            "name bert_model.encoder.layer.10.attention.self.value.bias p False\n",
            "name bert_model.encoder.layer.10.attention.output.dense.weight p False\n",
            "name bert_model.encoder.layer.10.attention.output.dense.bias p False\n",
            "name bert_model.encoder.layer.10.attention.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.10.attention.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.10.intermediate.dense.weight p False\n",
            "name bert_model.encoder.layer.10.intermediate.dense.bias p False\n",
            "name bert_model.encoder.layer.10.output.dense.weight p False\n",
            "name bert_model.encoder.layer.10.output.dense.bias p False\n",
            "name bert_model.encoder.layer.10.output.LayerNorm.weight p False\n",
            "name bert_model.encoder.layer.10.output.LayerNorm.bias p False\n",
            "name bert_model.encoder.layer.11.attention.self.query.weight p True\n",
            "name bert_model.encoder.layer.11.attention.self.query.bias p True\n",
            "name bert_model.encoder.layer.11.attention.self.key.weight p True\n",
            "name bert_model.encoder.layer.11.attention.self.key.bias p True\n",
            "name bert_model.encoder.layer.11.attention.self.value.weight p True\n",
            "name bert_model.encoder.layer.11.attention.self.value.bias p True\n",
            "name bert_model.encoder.layer.11.attention.output.dense.weight p True\n",
            "name bert_model.encoder.layer.11.attention.output.dense.bias p True\n",
            "name bert_model.encoder.layer.11.attention.output.LayerNorm.weight p True\n",
            "name bert_model.encoder.layer.11.attention.output.LayerNorm.bias p True\n",
            "name bert_model.encoder.layer.11.intermediate.dense.weight p True\n",
            "name bert_model.encoder.layer.11.intermediate.dense.bias p True\n",
            "name bert_model.encoder.layer.11.output.dense.weight p True\n",
            "name bert_model.encoder.layer.11.output.dense.bias p True\n",
            "name bert_model.encoder.layer.11.output.LayerNorm.weight p True\n",
            "name bert_model.encoder.layer.11.output.LayerNorm.bias p True\n",
            "name bert_model.pooler.dense.weight p True\n",
            "name bert_model.pooler.dense.bias p True\n",
            "name proj.0.weight p True\n",
            "name proj.0.bias p True\n",
            "name proj.2.weight p True\n",
            "name proj.2.bias p True\n",
            "name proj.4.weight p True\n",
            "name proj.4.bias p True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def val_acc(model, dev_loader, batch_size_dev):\n",
        "  model.eval()\n",
        "  num_correct = 0\n",
        "  for batch_idx, ( question, subt_text, a0, a1, a2, a3, a4, video_names, answer_idx) in enumerate(test_loader):\n",
        "    answer_idx = answer_idx.to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "      # # IF MODEL does not TAKES VIDEO INPUT\n",
        "\n",
        "      logits = model.forward(question, subt_text, a0, a1, a2, a3, a4, video_names)\n",
        "\n",
        "      \n",
        "    num_correct += int((torch.argmax(logits, axis=1) == answer_idx).sum())\n",
        "    acc = 100 * num_correct / ((batch_idx + 1) * batch_size_dev)\n",
        "\n",
        "  dev_acc = 100 * num_correct / (len(dev_loader) * batch_size_dev)\n",
        "\n",
        "  model.train()\n",
        "  return dev_acc"
      ],
      "metadata": {
        "id": "MEh2Hhvlmefj"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "optimizer = optim.Adam(tvqa_model.parameters(), lr=1e-3)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
      ],
      "metadata": {
        "id": "LIvDHgmTkg8l"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Fj4UDjvIvod",
        "outputId": "3969baec-6085-4574-94cb-340ba24f3af9"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Set the verbosity level as follows:\n",
        "\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "\n",
        "# bert_model.cuda()\n",
        "\n",
        "model_version=\"VCL_model.pth\"\n",
        "\n",
        "\n",
        "epoch = 0\n",
        "best_dev_acc = 0\n",
        "loss_epoch = 0\n",
        "\n",
        "while epoch < 100:\n",
        "  \n",
        "    num_correct = 0\n",
        "\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "    tvqa_model.train()\n",
        "\n",
        "\n",
        "    if os.path.exists(f'/content/drive/MyDrive/MMML/{model_version}'):\n",
        "        # model.load_state_dict(torch.load(f'{SAVE_PATH}{EXP_TAG}/model_saved_epoch{epoch-1}.pt')) \n",
        "\n",
        "        checkpoint = torch.load(f'/content/drive/MyDrive/MMML/{model_version}')\n",
        "        tvqa_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "    for batch_idx, ( question, subt_text, a0, a1, a2, a3, a4, video_names, answer_idx) in enumerate(train_loader):\n",
        "\n",
        "        logits = tvqa_model.forward(question, subt_text, a0, a1, a2, a3, a4, video_names)\n",
        "        answer_idx = answer_idx.to(\"cuda\")\n",
        "        loss = criterion(logits, answer_idx)\n",
        "        num_correct += int((torch.argmax(logits, axis=1) == answer_idx).sum())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_epoch += float(loss)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * num_correct / ((batch_idx + 1) * batch_size)),\n",
        "            loss=\"{:.04f}\".format(float(loss_epoch / (batch_idx + 1))),\n",
        "            num_correct=num_correct,\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        \n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "        # break\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    train_acc = 100 * num_correct / (len(train_loader) * batch_size)\n",
        "    dev_acc = val_acc(tvqa_model, test_loader, dev_batch_size)\n",
        "\n",
        "    print(f'Epoch {epoch} Loss {loss_epoch} train_acc {train_acc}, devacc {dev_acc}')\n",
        "\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': tvqa_model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss_epoch/len(train_loader),\n",
        "            },  f'/content/drive/MyDrive/MMML/{model_version}')\n",
        "    \n",
        "    if dev_acc > best_dev_acc:\n",
        "        best_dev_acc = dev_acc\n",
        "\n",
        "        torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': tvqa_model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss_epoch/len(train_loader),\n",
        "                },  f'/content/drive/MyDrive/MMML/best_dev_acc_{model_version}')\n",
        "\n",
        "        \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkZOjiffOp1m",
        "outputId": "e11220a9-bdb2-4c91-bf84-77743568940e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:   1%|          | 10/982 [01:58<3:10:49, 11.78s/it, acc=34.1667%, loss=1.5304, lr=0.0010, num_correct=82]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': tvqa_model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'loss': loss_epoch/len(train_loader),\n",
        "      },  f'/content/drive/MyDrive/MMML/bert_backprop.pth')"
      ],
      "metadata": {
        "id": "FVKJk5YUKhQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tvqa_model = TVQA_Multimodal()\n",
        "tvqa_model._freeze_bert_weights()\n",
        "# print(tvqa_model)\n",
        "# tvqa_model._free\n",
        "\n",
        "for name, p in tvqa_model.bert_model.named_parameters():\n",
        "    print(\"name\", name, \"p\", p.shape, \"p.requires_grad\", p.requires_grad)"
      ],
      "metadata": {
        "id": "2R8VlswM_db-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ojYxBI0QAfZO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}