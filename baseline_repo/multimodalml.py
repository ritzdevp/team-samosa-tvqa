# -*- coding: utf-8 -*-
"""MultimodalML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ujA3R6-CpmIsLCilOYkbaHdXRk8bDieJ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score

from google.colab import drive

drive.mount('/content/drive', force_remount=True)

# https://huggingface.co/docs/transformers/model_doc/bert
# https://huggingface.co/docs/transformers/training
# https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/training.ipynb

BASE_PATH = "/home/ubuntu/MML"

import json
import pysrt

TVQA_TRAIN_DICT_JSON = "/content/tvqa_qa_release/tvqa_train.jsonl"
TVQA_VAL_DICT_JSON = "/content/tvqa_qa_release/tvqa_val.jsonl"
TVQA_TEST_DICT_JSON = '/content/tvqa_qa_release/tvqa_test_public.jsonl'
SUBTITLE_PATH = '/content/tvqa_subtitles/'

# anno_tvqa_val_path = '/content/tvqa_qa_release/tvqa_val.jsonl'
# anno_tvqa_train_path = '/content/tvqa_qa_release/tvqa_train.jsonl'
# anno_tvqa_test_public_path = '/content/tvqa_qa_release/tvqa_test_public.jsonl'


anno_tvqa_val = []
anno_tvqa_test_public = []
anno_tvqa_train = []

with open(TVQA_VAL_DICT_JSON,'r',encoding='utf-8') as j:
   for line in j:
     anno_tvqa_val.append(json.loads(line))

with open(TVQA_TRAIN_DICT_JSON,'r',encoding='utf-8') as j:
   for line in j:
     anno_tvqa_train.append(json.loads(line))

with open(TVQA_TEST_DICT_JSON,'r',encoding='utf-8') as j:
   for line in j:
     anno_tvqa_test_public.append(json.loads(line))

print('# Train Samples', len(anno_tvqa_train))
print('# Validation Samples', len(anno_tvqa_val))
print('# Test Samples', len(anno_tvqa_test_public))



RAND_TRAIN_ID = 30000

RAND_INDEX = anno_tvqa_train[RAND_TRAIN_ID]['vid_name']
# subtitle_rand_entry = subtitles_dict[RAND_INDEX]

# subt_text = subtitle_rand_entry['sub_text']
# subt_start_time = subtitle_rand_entry['sub_time']

# filt_subt_text = [sent for sent in subt_text.split('<eos>')]
# assert len(filt_subt_text) == len(subt_start_time) 

for key, val in anno_tvqa_train[RAND_TRAIN_ID].items():
  print(f"Train ID: {RAND_TRAIN_ID}, Key: {key}, Value: {val}")

# subt_text = subt_text.replace('<eos>', '[SEP]')
# print(subt_text)

subtitle_path = SUBTITLE_PATH + anno_tvqa_train[RAND_TRAIN_ID]["vid_name"] + ".srt"
subs = pysrt.open(subtitle_path)
print(subs)

def read_tvqa_subtitles(srt_path: str):
  subtitle_path = SUBTITLE_PATH + anno_tvqa_train[RAND_TRAIN_ID]["vid_name"] + ".srt"
  subs = pysrt.open(subtitle_path)
  
  texts = []
  
  for sub in subs:
    line_text = sub.text.replace('(', '')
    line_text = line_text.replace(':)', ': ')
    line_text = line_text.replace('\n', '')
    texts.append(line_text)

  return " [SEP] ".join(texts)

subt_filt = read_tvqa_subtitles(anno_tvqa_train[RAND_TRAIN_ID]["vid_name"])
print("subt_filt", subt_filt)

import json

TRAIN_DICT_JSON = "/content/tvqa_plus/tvqa_plus_train.json"
VAL_DICT_JSON = '/content/tvqa_plus/tvqa_plus_val.json'
SUBTITLES_DICT_JSON = '/content/tvqa_plus/tvqa_plus_subtitles.json'


train_dict = {}
val_dict = {}
subtitles_dict = {}


with open(TRAIN_DICT_JSON) as f:
  train_dict = json.load(f)

with open(VAL_DICT_JSON) as f:
  val_dict = json.load(f)

with open(SUBTITLES_DICT_JSON) as f:
  subtitles_dict = json.load(f)

print('# Train Samples', len(train_dict))
print('# Validation Samples', len(val_dict))

print('Length Subtitles Dict ', len(subtitles_dict))
print('subtitles_dict keys', subtitles_dict.keys())

RAND_TRAIN_ID = 20


RAND_INDEX = train_dict[RAND_TRAIN_ID]['vid_name']
subtitle_rand_entry = subtitles_dict[RAND_INDEX]

subt_text = subtitle_rand_entry['sub_text']
subt_start_time = subtitle_rand_entry['sub_time']

filt_subt_text = [sent for sent in subt_text.split('<eos>')]
# assert len(filt_subt_text) == len(subt_start_time) 

for key, val in train_dict[RAND_TRAIN_ID].items():
  print(f"Train ID: {RAND_TRAIN_ID}, Key: {key}, Value: {val}")

subt_text = subt_text.replace('<eos>', '[SEP]')
print(subt_text)

# for start_time, text in zip(subt_start_time, filt_subt_text):
#   print(start_time, text)

"""#### **ROBERTA PRE-TRAINED MODEL**

## **BERT MODEL**
"""

! pip install transformers datasets

# https://huggingface.co/docs/transformers/model_doc/bert

from transformers import BertTokenizer, BertModel
import torch

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased",  output_hidden_states = True)
bert = BertModel.from_pretrained("bert-base-uncased",  output_hidden_states = True)

def get_bert_embeddings(model, texts):
    """Get embeddings from an embedding model
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model=model.to(device)
    texts = ["[CLS] " + text + " [SEP]" for text in texts]
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True).to(device)
    output = model(**inputs)

    # https://stackoverflow.com/questions/67703260/xlm-bert-sequence-outputs-to-pooled-output-with-weighted-average-pooling
    # For Sentence embedding, [batch_size, seq_len, dim] --> use 'cls0' token or use 'pooler_output;. We use pooler_output
    hidden_states = output.last_hidden_state
    return hidden_states

# Text corpus
##############
# These sentences show the different
# forms of the word 'bank' to show the
# value of contextualized embeddings

texts = ("bank",
         "The river bank was flooded.",
         "The bank vault was robust.",
         "He had to bank on her for support.",
         "The bank was out of money.",
         "The bank teller was a man.")

# Getting embeddings for the target
# word in all given contexts
target_word_embeddings = []

# for text in texts:
list_token_embeddings = get_bert_embeddings(bert, texts)
print('list_token_embeddings shape', list_token_embeddings.shape)



"""## **DataLoader Class**"""

class TVQA(torch.utils.data.Dataset):

    SUBTITLE_PATH = '/content/tvqa_subtitles/'

    def __init__(self, dataset='train'):
        # sample represent how many npy files will be preloaded for one __getitem__ call
        TVQA_TRAIN_DICT_JSON = "/content/tvqa_qa_release/tvqa_train.jsonl"
        TVQA_VAL_DICT_JSON = "/content/tvqa_qa_release/tvqa_val.jsonl"
        TVQA_TEST_DICT_JSON = '/content/tvqa_qa_release/tvqa_test_public.jsonl'

        self.dataset = dataset

        anno_tvqa_val = []
        anno_tvqa_test_public = []
        anno_tvqa_train = []

        with open(TVQA_VAL_DICT_JSON,'r',encoding='utf-8') as j:
          for line in j:
            anno_tvqa_val.append(json.loads(line))

        with open(TVQA_TRAIN_DICT_JSON,'r',encoding='utf-8') as j:
          for line in j:
            anno_tvqa_train.append(json.loads(line))

        with open(TVQA_TEST_DICT_JSON,'r',encoding='utf-8') as j:
          for line in j:
            anno_tvqa_test_public.append(json.loads(line))


        self.target_dict = {}
        if self.dataset == 'train':
          self.target_dict = anno_tvqa_train
        elif self.dataset == 'val':
          self.target_dict = anno_tvqa_val
        else:
          raise Exception(f"Invalid dataset passed {self.dataset}")

    @staticmethod
    def read_tvqa_subtitles(srt_path: str):
      subs = pysrt.open(srt_path)
      
      texts = []
      
      for sub in subs:
        line_text = sub.text.replace('(', '')
        line_text = line_text.replace(':)', ': ')
        line_text = line_text.replace('\n', '')
        texts.append(line_text)

      return " [SEP] ".join(texts)
      
    def __len__(self):
        return len(self.target_dict)
        
    def __getitem__(self, i):

      question = self.target_dict[i]['q']
    
      a0       = self.target_dict[i]['a0']
      a1       = self.target_dict[i]['a1']
      a2       = self.target_dict[i]['a2']
      a3       = self.target_dict[i]['a3']
      a4       = self.target_dict[i]['a4']

      answer_idx = int(self.target_dict[i]['answer_idx'])      
      subtitle_path = TVQA.SUBTITLE_PATH + self.target_dict[i]["vid_name"] + ".srt"

      subt_text = TVQA.read_tvqa_subtitles(subtitle_path)

      # ans_ohe = torch.zeros((5, ))
      # ans_ohe[answer_idx] = 1

      return question, subt_text, a0, a1, a2, a3, a4, answer_idx

import json

class TVQAPlus(torch.utils.data.Dataset):
    def __init__(self, dataset='train'):
        # sample represent how many npy files will be preloaded for one __getitem__ call
        TRAIN_DICT_JSON = "/content/tvqa_plus/tvqa_plus_train.json"
        VAL_DICT_JSON = '/content/tvqa_plus/tvqa_plus_val.json'
        SUBTITLES_DICT_JSON = '/content/tvqa_plus/tvqa_plus_subtitles.json'

        self.dataset = dataset

        train_dict = {}
        val_dict = {}
        subtitles_dict = {}

        with open(TRAIN_DICT_JSON) as f:
          train_dict = json.load(f)

        with open(VAL_DICT_JSON) as f:
          val_dict = json.load(f)

        with open(SUBTITLES_DICT_JSON) as f:
          self.subtitles_dict = json.load(f)

        self.target_dict = {}
        if self.dataset == 'train':
          self.target_dict = train_dict
        elif self.dataset == 'val':
          self.target_dict = val_dict
        else:
          raise Exception(f"Invalid dataset passed {self.dataset}")

      
    def __len__(self):
        return len(self.target_dict)
        
    def __getitem__(self, i):

      question = self.target_dict[i]['q']
    
      a0       = self.target_dict[i]['a0']
      a1       = self.target_dict[i]['a1']
      a2       = self.target_dict[i]['a2']
      a3       = self.target_dict[i]['a3']
      a4       = self.target_dict[i]['a4']

      answer_idx = int(self.target_dict[i]['answer_idx'])
      
      subtitle_key = self.target_dict[i]['vid_name']
      subtitle = self.subtitles_dict[subtitle_key]
      subt_text = subtitle['sub_text']
      subt_text = subt_text.replace('<eos>', '[SEP]')

      # ans_ohe = torch.zeros((5, ))
      # ans_ohe[answer_idx] = 1

      return question, subt_text, a0, a1, a2, a3, a4, answer_idx

"""#***MODEL*** """

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score

class TVQAQAModel(torch.nn.Module):

  def __init__(self,
               q_dim: int=768,
               a_dim: int=768,
               subt_dim: int=768,
               num_ans: int=5,
               att_dim: int=64,
               ):
    super(TVQAQAModel, self).__init__()

    hidden_proj_dim = 256

    quest_proj = [nn.Linear(q_dim, hidden_proj_dim),
                  nn.GELU()]

    ans_proj = [nn.Linear(a_dim, hidden_proj_dim),
                nn.GELU()]

    subt_proj = [nn.Linear(subt_dim, hidden_proj_dim),
                 nn.GELU()]
                 


    cls_layer = [nn.Linear(att_dim, 1)]

    self.quest_proj = nn.Sequential(*quest_proj)
    self.ans_proj   = nn.Sequential(*ans_proj)
    self.subt_proj  = nn.Sequential(*subt_proj)

    self.query_proj = nn.Sequential(nn.Linear(hidden_proj_dim, att_dim))
    self.value_proj = nn.Sequential(nn.Linear(hidden_proj_dim, att_dim))
    self.key_proj   = nn.Sequential(nn.Linear(hidden_proj_dim, att_dim))

    self.attention = nn.MultiheadAttention(embed_dim=att_dim, num_heads=1, batch_first=True)
    self.cls_layer = nn.Sequential(*cls_layer)


  def forward(self, question, a1, a2, a3, a4, a5, subt):

    # ans_concat = torch.cat((a1, a2, a3, a4, a5), dim=1)

    q_fwd = self.quest_proj(question)
    subt_fwd = self.subt_proj(subt)

    ans_one_fwd = self.ans_proj(a1)
    ans_two_fwd = self.ans_proj(a2)
    ans_three_fwd = self.ans_proj(a3)
    ans_four_fwd  = self.ans_proj(a4)
    ans_five_fwd = self.ans_proj(a5)

    # print("q_fwd", q_fwd.shape)
    # print("subt_fwd", subt_fwd.shape)

    # print("ans_one_fwd", ans_one_fwd.shape)
    # print("ans_two_fwd", ans_two_fwd.shape)
    # print("ans_three_fwd", ans_three_fwd.shape)
    # print("ans_four_fwd", ans_four_fwd.shape)
    # print("ans_five_fwd", ans_five_fwd.shape)
    # print("\n")

    q_ans_one_subt_concat = torch.cat((q_fwd, ans_one_fwd, subt_fwd), dim=1)
    q_ans_two_subt_concat = torch.cat((q_fwd, ans_two_fwd, subt_fwd), dim=1)
    q_ans_three_subt_concat = torch.cat((q_fwd, ans_three_fwd, subt_fwd), dim=1)
    q_ans_four_subt_concat  = torch.cat((q_fwd, ans_four_fwd, subt_fwd), dim=1)
    q_ans_five_subt_concat  = torch.cat((q_fwd, ans_five_fwd, subt_fwd), dim=1)

    # print("q_ans_one_subt_concat", q_ans_one_subt_concat.shape)
    # print("q_ans_two_subt_concat", q_ans_two_subt_concat.shape)
    # print("q_ans_three_subt_concat", q_ans_three_subt_concat.shape)
    # print("q_ans_four_subt_concat", q_ans_four_subt_concat.shape)
    # print("q_ans_five_subt_concat", q_ans_five_subt_concat.shape)
    # print("\n")

    att_one_query = self.query_proj(q_ans_one_subt_concat)
    att_one_key   = self.key_proj(q_ans_one_subt_concat)
    att_one_val   = self.value_proj(q_ans_one_subt_concat)

    # print("att_one_query", att_one_query.shape)
    # print("att_one_key", att_one_key.shape)
    # print("att_one_val", att_one_val.shape)
    # print("\n")

    att_two_query = self.query_proj(q_ans_two_subt_concat)
    att_two_key   = self.key_proj(q_ans_two_subt_concat)
    att_two_val   = self.value_proj(q_ans_two_subt_concat)

    # print("att_two_query", att_two_query.shape)
    # print("att_two_key", att_two_key.shape)
    # print("att_two_val", att_two_val.shape)
    # print("\n")

    att_three_query = self.query_proj(q_ans_three_subt_concat)
    att_three_key   = self.key_proj(q_ans_three_subt_concat)
    att_three_val    = self.value_proj(q_ans_three_subt_concat)

    # print("att_three_query", att_three_query.shape)
    # print("att_three_key", att_three_key.shape)
    # print("att_three_val", att_three_val.shape)
    # print("\n")

    att_four_query  = self.query_proj(q_ans_four_subt_concat)
    att_four_key    = self.key_proj(q_ans_four_subt_concat)
    att_four_val    = self.value_proj(q_ans_four_subt_concat)

    # print("att_four_query", att_four_query.shape)
    # print("att_four_key", att_four_key.shape)
    # print("att_four_val", att_four_val.shape)
    # print("\n")

    att_five_query  = self.query_proj(q_ans_five_subt_concat)
    att_five_key    = self.key_proj(q_ans_five_subt_concat)
    att_five_val    = self.value_proj(q_ans_five_subt_concat)

    # print("att_five_query", att_five_query.shape)
    # print("att_five_key", att_five_key.shape)
    # print("att_five_val", att_five_val.shape)
    # print("\n")

    att_one_fwd, att_one_weight = self.attention(query=att_one_query, key=att_one_key, value=att_one_val)
    att_one_fwd_pool = torch.max(att_one_fwd, dim=1).values
    score_one        = self.cls_layer(att_one_fwd_pool)

    # print("att_one_fwd", att_one_fwd.shape)
    # print("att_one_fwd_pool", att_one_fwd_pool.shape)
    # print("score_one", score_one.shape, "score_one", score_one)
    # print("\n")

    att_two_fwd, att_two_weight = self.attention(query=att_two_query, key=att_two_key, value=att_two_val)
    att_two_fwd_pool = torch.max(att_two_fwd, dim=1).values
    score_two        = self.cls_layer(att_two_fwd_pool)

    # print("att_two_fwd", att_two_fwd.shape)
    # print("att_two_fwd_pool", att_two_fwd_pool.shape)
    # print("score_two", score_two.shape, "score_two", score_one)
    # print("\n")


    att_three_fwd, att_three_weight = self.attention(query=att_three_query, key=att_three_key, value=att_three_val)
    att_three_fwd_pool = torch.max(att_three_fwd, dim=1).values
    score_three      = self.cls_layer(att_three_fwd_pool)

    # print("att_three_fwd", att_three_fwd.shape)
    # print("att_three_fwd_pool", att_three_fwd_pool.shape)
    # print("score_three", score_three.shape, "score_three", score_three)
    # print("\n")

    att_four_fwd, att_four_weight = self.attention(query=att_four_query, key=att_four_key, value=att_four_val)
    att_four_fwd_pool  = torch.max(att_four_fwd, dim=1).values
    score_four         = self.cls_layer(att_four_fwd_pool)

    # print("att_four_fwd", att_four_fwd.shape)
    # print("att_four_fwd_pool", att_four_fwd_pool.shape)
    # print("score_four", score_four.shape, "score_four", score_four)
    # print("\n")

    att_five_fwd, att_five_weight = self.attention(query=att_five_query, key=att_five_key, value=att_five_val)
    att_five_fwd_pool  = torch.max(att_five_fwd, dim=1).values
    score_five         = self.cls_layer(att_five_fwd_pool)


    # print("att_five_fwd", att_five_fwd.shape)
    # print("att_five_fwd_pool", att_five_fwd_pool.shape)
    # print("score_five", score_five.shape, "score_five", score_five)
    # print("\n")

    # att_fwd, att_weights = self.quest_ans_subt_att(query=att_query, key=att_key, value=att_val)
    # att_fwd_time_pool = torch.max(att_fwd, dim=1).values
    # print('att_fwd_time_pool', att_fwd_time_pool.shape)

    # logits = self.cls_layer(att_fwd_time_pool)
    # print('logits', logits.shape)
    logits = torch.cat((score_one, score_two, score_three, score_four, score_five), dim=1)

    # print("logit scores ", logits.shape)

    return logits

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score

class TVQAQAModelTransformer(torch.nn.Module):

  def __init__(self,
               q_dim: int=768,
               a_dim: int=768,
               subt_dim: int=768,
               num_ans: int=5,
               att_dim: int=256,
               n_heads: int=8,
               enc_layers: int=6
               ):
    super(TVQAQAModelTransformer, self).__init__()

    quest_proj = [nn.Linear(q_dim, att_dim),
                  nn.GELU()]

    ans_proj = [nn.Linear(a_dim, att_dim),
                nn.GELU()]

    subt_proj = [nn.Linear(subt_dim, att_dim),
                 nn.GELU()]
    self.quest_proj = nn.Sequential(*quest_proj)
    self.ans_proj   = nn.Sequential(*ans_proj)
    self.subt_proj  = nn.Sequential(*subt_proj)


    encoder_layer = nn.TransformerEncoderLayer(d_model=att_dim, nhead=n_heads, dim_feedforward=att_dim, batch_first=True)
    self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=enc_layers)

    cls_layer = [nn.Linear(att_dim, 1)]
    self.cls_layer = nn.Sequential(*cls_layer)


  def forward(self, question, a1, a2, a3, a4, a5, subt):

    q_fwd = self.quest_proj(question)
    subt_fwd = self.subt_proj(subt)

    ans_one_fwd = self.ans_proj(a1)
    ans_two_fwd = self.ans_proj(a2)
    ans_three_fwd = self.ans_proj(a3)
    ans_four_fwd  = self.ans_proj(a4)
    ans_five_fwd = self.ans_proj(a5)

    # print("q_fwd", q_fwd.shape)
    # print("subt_fwd", subt_fwd.shape)

    # print("ans_one_fwd", ans_one_fwd.shape)
    # print("ans_two_fwd", ans_two_fwd.shape)
    # print("ans_three_fwd", ans_three_fwd.shape)
    # print("ans_four_fwd", ans_four_fwd.shape)
    # print("ans_five_fwd", ans_five_fwd.shape)
    # print("\n")

    q_ans_one_subt_concat = torch.cat((subt_fwd, q_fwd, ans_one_fwd), dim=1)
    q_ans_two_subt_concat = torch.cat((subt_fwd, q_fwd, ans_two_fwd), dim=1)
    q_ans_three_subt_concat = torch.cat((subt_fwd, q_fwd, ans_three_fwd), dim=1)
    q_ans_four_subt_concat  = torch.cat((subt_fwd, q_fwd, ans_four_fwd), dim=1)
    q_ans_five_subt_concat  = torch.cat((subt_fwd, q_fwd, ans_five_fwd), dim=1)

    # print("q_ans_one_subt_concat", q_ans_one_subt_concat.shape)
    # print("q_ans_two_subt_concat", q_ans_two_subt_concat.shape)
    # print("q_ans_three_subt_concat", q_ans_three_subt_concat.shape)
    # print("q_ans_four_subt_concat", q_ans_four_subt_concat.shape)
    # print("q_ans_five_subt_concat", q_ans_five_subt_concat.shape)
    # print("\n")


    att_one_fwd = self.transformer_encoder(q_ans_one_subt_concat)
    att_one_fwd_pool = torch.max(att_one_fwd, dim=1).values
    score_one        = self.cls_layer(att_one_fwd_pool)

    # print("att_one_fwd", att_one_fwd.shape)
    # print("att_one_fwd_pool", att_one_fwd_pool.shape)
    # print("score_one", score_one.shape, "score_one", score_one)
    # print("\n")

    att_two_fwd = self.transformer_encoder(q_ans_two_subt_concat)
    att_two_fwd_pool = torch.max(att_two_fwd, dim=1).values
    score_two        = self.cls_layer(att_two_fwd_pool)

    # print("att_two_fwd", att_two_fwd.shape)
    # print("att_two_fwd_pool", att_two_fwd_pool.shape)
    # print("score_two", score_two.shape, "score_two", score_two)
    # print("\n")


    att_three_fwd = self.transformer_encoder(q_ans_three_subt_concat)
    att_three_fwd_pool = torch.max(att_three_fwd, dim=1).values
    score_three      = self.cls_layer(att_three_fwd_pool)

    # print("att_three_fwd", att_three_fwd.shape)
    # print("att_three_fwd_pool", att_three_fwd_pool.shape)
    # print("score_three", score_three.shape, "score_three", score_three)
    # print("\n")

    att_four_fwd = self.transformer_encoder(q_ans_four_subt_concat)
    att_four_fwd_pool  = torch.max(att_four_fwd, dim=1).values
    score_four         = self.cls_layer(att_four_fwd_pool)

    # print("att_four_fwd", att_four_fwd.shape)
    # print("att_four_fwd_pool", att_four_fwd_pool.shape)
    # print("score_four", score_four.shape, "score_four", score_four)
    # print("\n")

    att_five_fwd = self.transformer_encoder(q_ans_five_subt_concat)
    att_five_fwd_pool  = torch.max(att_five_fwd, dim=1).values
    score_five         = self.cls_layer(att_five_fwd_pool)


    # print("att_five_fwd", att_five_fwd.shape)
    # print("att_five_fwd_pool", att_five_fwd_pool.shape)
    # print("score_five", score_five.shape, "score_five", score_five)
    # print("\n")

    logits = torch.cat((score_one, score_two, score_three, score_four, score_five), dim=1)

    return logits



# del tvqa_model
# torch.cuda.empty_cache()
# !nvidia-smi

a = torch.ones((3,10))
b = torch.ones((3,12))
d = torch.ones((3,4))
c = torch.cat((a, b, d), dim=1)
print(c.shape)

import tqdm
dev_items = TVQA(dataset='val')

batch_size_dev = 4
dev_loader = torch.utils.data.DataLoader(dev_items, batch_size=batch_size_dev, shuffle=False)

# batch_bar = tqdm(total=len(dev_loader), dynamic_ncols=True, leave=False, position=0)

# train_items = TVQAPlus(dataset='train')
train_items = TVQA('train')
batch_size = 2
train_loader = torch.utils.data.DataLoader(train_items, batch_size=batch_size, shuffle=True)


# tvqa_model = TVQAQAModelTransformer()
tvqa_model = TVQAQAModel()
tvqa_model.cuda()

for batch_idx, (question, subt_text, a0, a1, a2, a3, a4, ans_ohe) in enumerate(train_loader):
  ans_ohe = ans_ohe.cuda()

  print('subt_text zero', subt_text[0])
  print('subt_text  one', subt_text[1])

  print('question', question)
  print('subt text', subt_text)
  print('a0', len(a0))


  quest_embed = get_bert_embeddings(model=bert, texts=question)
  subt_text_embed = get_bert_embeddings(model=bert, texts=subt_text)
  a0_embed = get_bert_embeddings(model=bert, texts=a0)
  a1_embed = get_bert_embeddings(model=bert, texts=a1)
  a2_embed = get_bert_embeddings(model=bert, texts=a2)
  a3_embed = get_bert_embeddings(model=bert, texts=a3)
  a4_embed = get_bert_embeddings(model=bert, texts=a4)


  print('question embedding ', quest_embed.shape)
  print('subt text_embed', subt_text_embed.shape)
  print('a0 shape', a0_embed.shape)
  print('a1 shape', a1_embed.shape)
  print('a2 shape', a2_embed.shape)
  print('a3 shape', a3_embed.shape)
  print('a4 shape', a4_embed.shape)

  logits = tvqa_model.forward(question=quest_embed, 
                                a1=a0_embed, 
                                a2=a1_embed, 
                                a3=a2_embed,
                                a4=a3_embed, 
                                a5=a4_embed,
                                subt=subt_text_embed)
  print(logits)
  print('logit shape', logits.shape)

  break

def val_acc(model):
  model.eval()
  num_correct = 0
  for batch_idx, (question, subt_text, a0, a1, a2, a3, a4, ans_ohe) in enumerate(dev_loader):
    ans_ohe = ans_ohe.cuda()
    quest_embed = get_bert_embeddings(model=bert, texts=question)
    subt_text_embed = get_bert_embeddings(model=bert, texts=subt_text)
    a0_embed = get_bert_embeddings(model=bert, texts=a0)
    a1_embed = get_bert_embeddings(model=bert, texts=a1)
    a2_embed = get_bert_embeddings(model=bert, texts=a2)
    a3_embed = get_bert_embeddings(model=bert, texts=a3)
    a4_embed = get_bert_embeddings(model=bert, texts=a4)

    with torch.no_grad():
      logits = model.forward(question=quest_embed, 
                                  a1=a0_embed, 
                                  a2=a1_embed, 
                                  a3=a2_embed,
                                  a4=a3_embed, 
                                  a5=a4_embed,
                                  subt=subt_text_embed)
      
    num_correct += int((torch.argmax(logits, axis=1) == ans_ohe).sum())
    # print(acc="{:.04f}%".format(100 * num_correct / ((batch_idx + 1) * batch_size)))
    acc = 100 * num_correct / ((batch_idx + 1) * batch_size_dev)
    # print('accuracy is ', acc)

  dev_acc = 100 * num_correct / (len(dev_loader) * batch_size_dev)

  model.train()
  return dev_acc

from tqdm import tqdm
import os

# train_items = TVQAPlus(dataset='train')
train_items = TVQA('train')

batch_size = 16
train_loader = torch.utils.data.DataLoader(train_items, batch_size=batch_size, shuffle=True)

# tvqa_model = TVQAQAModelTransformer()
tvqa_model = TVQAQAModel()
tvqa_model.cuda()

optimizer = optim.Adam(tvqa_model.parameters(), lr=3e-3)
criterion = torch.nn.CrossEntropyLoss()
scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)
print('tvqa_model', tvqa_model)
model_version='model_attention_tvqa_v1.pt'
epoch = 0
best_dev_acc = 0
while epoch < 100:

  loss_epoch = 0
  num_correct = 0
  optimizer.zero_grad()
  tvqa_model.train()

  batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')

  if os.path.exists(f'/content/drive/MyDrive/MultiModalExp/{model_version}'):
    # model.load_state_dict(torch.load(f'{SAVE_PATH}{EXP_TAG}/model_saved_epoch{epoch-1}.pt')) 

    checkpoint = torch.load(f'/content/drive/MyDrive/MultiModalExp/{model_version}')
    tvqa_model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    epoch = checkpoint['epoch'] + 1
    
  for batch_idx, (question, subt_text, a0, a1, a2, a3, a4, ans_ohe) in enumerate(train_loader):
    ans_ohe = ans_ohe.cuda()

    quest_embed = get_bert_embeddings(model=bert, texts=question)
    subt_text_embed = get_bert_embeddings(model=bert, texts=subt_text)
    a0_embed = get_bert_embeddings(model=bert, texts=a0)
    a1_embed = get_bert_embeddings(model=bert, texts=a1)
    a2_embed = get_bert_embeddings(model=bert, texts=a2)
    a3_embed = get_bert_embeddings(model=bert, texts=a3)
    a4_embed = get_bert_embeddings(model=bert, texts=a4)

    # print('quest_embed', quest_embed.shape)
    # print('subt_text_embed', subt_text_embed.shape)
    # print('a0_embed', a0_embed.shape)
    # print('a1_embed', a1_embed.shape)
    # print('a2_embed', a2_embed.shape)
    # print('a3_embed', a3_embed.shape)
    # print('a4_embed', a4_embed.shape)


    logits = tvqa_model.forward(question=quest_embed, 
                                a1=a0_embed, 
                                a2=a1_embed, 
                                a3=a2_embed,
                                a4=a3_embed, 
                                a5=a4_embed,
                                subt=subt_text_embed)
    

    batch_bar.set_postfix(
        acc="{:.04f}%".format(100 * num_correct / ((batch_idx + 1) * batch_size)),
        loss="{:.04f}".format(float(loss_epoch / (batch_idx + 1))),
        num_correct=num_correct,
        lr="{:.04f}".format(float(optimizer.param_groups[0]['lr'])))
    

    loss = criterion(logits, ans_ohe)
    num_correct += int((torch.argmax(logits, axis=1) == ans_ohe).sum())
    # print('ans_ohe', ans_ohe.shape)
    # print('torch.argmax(logits, axis=1)', torch.argmax(logits, axis=1).shape)
    # print(f"epoch {epoch} batch {batch_idx} logits {logits} ans_ohe {ans_ohe} loss_epoch {loss_epoch}")


    loss.backward()
    optimizer.step()
    loss_epoch += float(loss)
    optimizer.zero_grad()

    batch_bar.update() # Update tqdm bar



  batch_bar.close() # You need this to close the tqdm bar
  torch.save({
          'epoch': epoch,
          'model_state_dict': tvqa_model.state_dict(),
          'optimizer_state_dict': optimizer.state_dict(),
          'loss': loss,
          },  f'/content/drive/MyDrive/MultiModalExp/{model_version}')

  train_acc = 100 * num_correct / (len(train_loader) * batch_size)
  dev_acc = val_acc(tvqa_model)

  if dev_acc > best_dev_acc:
    best_dev_acc = dev_acc
    torch.save({
            'epoch': epoch,
            'model_state_dict': tvqa_model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'dev_acc': dev_acc,
            'train_acc': train_acc,
            'loss': loss,
            },  f'/content/drive/MyDrive/MultiModalExp/best_dev_acc_{best_dev_acc}_{model_version}')

  print(f'Epoch {epoch} Loss {loss_epoch} train_acc {train_acc}, devacc {dev_acc}')
  epoch += 1

  scheduler.step()

# tvqa_model = TVQAQAModel()
# tvqa_model.cuda()

# checkpoint = torch.load(f'/content/drive/MyDrive/MultiModalExp/{model_version}')
# tvqa_model.load_state_dict(checkpoint['model_state_dict'])
# # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
# epoch = checkpoint['epoch'] + 1


dev_acc = val_acc(tvqa_model)


print('dev acc', dev_acc)

input = torch.randn(3, 5, requires_grad=True)
target = torch.empty(3, dtype=torch.long).random_(5)
output = loss(input, target)





"""#***SPAUC*** 

"""

RAND_TRAIN_ID = 230


RAND_INDEX = train_dict[RAND_TRAIN_ID]['vid_name']
subtitle_rand_entry = subtitles_dict[RAND_INDEX]

subt_text = subtitle_rand_entry['sub_text']
subt_start_time = subtitle_rand_entry['sub_time']

filt_subt_text = [sent for sent in subt_text.split('<eos>')]
# assert len(filt_subt_text) == len(subt_start_time) 

for key, val in train_dict[RAND_TRAIN_ID].items():
  print(f"Train ID: {RAND_TRAIN_ID}, Key: {key}, Value: {val}")

# subt_text = subt_text.replace('<eos>', '[SEP]')
print(subt_text)

# for start_time, text in zip(subt_start_time, filt_subt_text):
#   print(start_time, text)

! pip install spacy==2.3.2

! python -m spacy download en_core_web_sm

import spacy
from spacy.lang.en import English
from spacy.pipeline import EntityRuler


import spacy
from spacy import displacy

NER = spacy.load("en_core_web_sm")

raw_text="Sheldon loves Penny."
text1= NER(raw_text)
for word in text1.ents:
    print(word.text,word.label_)

nlp = spacy.load("en_core_web_sm")

ruler = EntityRuler(nlp)
# patterns = self.generate_entity_train_data()
# ruler.add_patterns(patterns)
nlp.add_pipe(ruler)

for word in name_doc:
  print('word', word, 'stop word', word.is_stop, "pos_ ", word.pos_ )

import numpy as np
a=np.array([0.0009, 0.0016, 0.0021, 0.0024, 0.0025, 0.0024, 0.0021, 0.0016, 0.0009])
a_norm = a/np.sum(a)
a_sym = ['-', 'a', '-', 'b', '-', 'b', '-', 'a', '-']
print(a_norm)

div_dict = {}

for i in range(0, len(a_sym)):
  div_dict[a_sym[i]] =  div_dict.get(a_sym[i], 0) + a_norm[i]

print(div_dict[])